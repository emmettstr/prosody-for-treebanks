{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "\n",
    "import conll\n",
    "import numpy as np\n",
    "import parselmouth\n",
    "import tgt\n",
    "\n",
    "from nltk.corpus import cmudict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SLAM features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dico2FeatureString(dico):\n",
    "    \"\"\"Transforms a dictionary of Conll features into a string\"\"\"\n",
    "    feature = [\"=\".join([key, str(dico[key])]) for key in dico]\n",
    "    feature = \"|\".join(feature)\n",
    "    return feature\n",
    "\n",
    "def build_feature_dico(misc_features_string):\n",
    "    \"\"\"Turns a string of CONLL features into a callable dictionary\"\"\"\n",
    "    feature_dico = {}\n",
    "    for feature in misc_features_string.split(\"|\"):\n",
    "        key, value = feature.split(\"=\")\n",
    "        feature_dico[key] = value\n",
    "    return feature_dico\n",
    "\n",
    "def confirm_alignment(ref_interval, target_interval):\n",
    "    \"\"\"Takes two pitchtier intervals as entry and returns whether or not there is a temporal overlap, as well as the nature of that overlap.\"\"\"\n",
    "    \n",
    "    left_overlap = False\n",
    "    right_overlap = False\n",
    "\n",
    "    \n",
    "    #print(ref_interval, target_interval)\n",
    "    if ref_interval.text == \"\":\n",
    "        # print(\"empty string\") # debug\n",
    "        return False, left_overlap, right_overlap  # disregard pauses\n",
    "\n",
    "    if round(target_interval.end_time - ref_interval.start_time,3) <= 0.001:\n",
    "        #print(\"A\") # debug\n",
    "        return False, left_overlap, right_overlap  \n",
    "    \n",
    "    elif round(target_interval.start_time - ref_interval.end_time,3) >= -0.001:\n",
    "        #print(\"B\") # debug \n",
    "        return False, left_overlap, right_overlap  \n",
    "    \n",
    "    if  round(ref_interval.start_time - target_interval.start_time, 3) >= 0.01:\n",
    "        #print(\"C\", str(ref_interval.start_time - target_interval.start_time)) # debug\n",
    "        left_overlap = True\n",
    "    if round(target_interval.end_time - ref_interval.end_time, 3)  >= 0.01:\n",
    "        #print(\"D\") # debug\n",
    "        right_overlap = True\n",
    "\n",
    "    return True, left_overlap, right_overlap \n",
    "\n",
    "\n",
    "\n",
    "def vowel_final(word_string, dico):\n",
    "    \"\"\"Takes as entry an orthographic word string and a pronunciation dictionary, and confirms whether or not the canonical pronunciation of a that word ends in a vowel\"\"\"\n",
    "    vowels=\"aeiouAEIOU\" \n",
    "    if word_string == \"\":\n",
    "        return False\n",
    "    try:\n",
    "        phonemes = pronunciation[word_string.lower()][0]\n",
    "        if phonemes[-1][0] in vowels:\n",
    "            return True\n",
    "    except:\n",
    "        if word_string[-1] in vowels:\n",
    "            return True\n",
    "    return False\n",
    "        \n",
    "def extractProsodicAnnotation(textgrid, label_tier, label_tier2, token_tier, word_text_tier, syllable_transcription_tier):\n",
    "    \"\"\"\n",
    "    Extracts prosodic information from a .TextGrid file containing SLAM annoations and returns a dictionary of annotations.\n",
    "\n",
    "    Takes as entry:\n",
    "    * A textgrid file containing tiers corresponding to the following elements.\n",
    "    * Name of the tier containing phonetic transcriptions of syllables (syllable_transcription_tier)\n",
    "    * Names of two tiers containing global (label_tier) and local (label_tier2) SLAM annotations of syllables \n",
    "    * Name of the tier containing a token-level alignment and numeric ID in format 2:13 (utterance two, token 13) (token_tier)\n",
    "    * Name of tier containing orthographic transcription of token (word_text_tier)\n",
    "    \"\"\"\n",
    "\n",
    "    pronunciation = cmudict.dict()\n",
    "    dico = {}\n",
    "    \n",
    "    try: textgrid_object = tgt.read_textgrid(textgrid, include_empty_intervals=True, encoding=\"utf-8\")\n",
    "    except: textgrid_object = tgt.read_textgrid(textgrid, include_empty_intervals=True, encoding=\"utf-16\")\n",
    "    label_obj = textgrid_object.get_tier_by_name(label_tier)\n",
    "    label_obj2 = textgrid_object.get_tier_by_name(label_tier2)\n",
    "    token_object = textgrid_object.get_tier_by_name(token_tier)  \n",
    "    word_text_object = textgrid_object.get_tier_by_name(word_text_tier)\n",
    "    syllable_transcription_object = textgrid_object.get_tier_by_name(syllable_transcription_tier)\n",
    "    \n",
    "    i=0\n",
    "    last_i = 0\n",
    "    for index, token in enumerate(token_object): \n",
    "        \n",
    "        skip_features = False\n",
    "        \n",
    "        alignment_found = False\n",
    "        #alignment = False\n",
    "        syl_number = 1\n",
    "        \n",
    "        if not token.text: continue\n",
    "        \n",
    "        dico[token.text] = {}\n",
    "        dico[token.text][\"AlignBegin\"] = str(int(token.start_time*1000))\n",
    "        dico[token.text][\"AlignEnd\"] = str(int(token.end_time*1000))\n",
    "        \n",
    "        ### Debugging/holdover from earlier version ###\n",
    "        #dico[token.text][\"LeftOverlap\"] = \"False\"\n",
    "        #dico[token.text][\"RightOverlap\"] = \"False\"\n",
    "        \n",
    "        alignment, left_overlap, right_overlap = confirm_alignment(token, label_obj[i])\n",
    "        #print(alignment, left_overlap, right_overlap) # debug\n",
    "        #print(token) # debug \n",
    "        while not alignment and i < len(label_obj) - 1:\n",
    "            i+=1 \n",
    "            #print(i) # debug \n",
    "            alignment, left_overlap, right_overlap = confirm_alignment(token, label_obj[i])\n",
    "            #print(alignment, left_overlap, right_overlap) # debug\n",
    "\n",
    "        if not alignment:\n",
    "            # If no alignment is found, skip the token and move on to the next one\n",
    "            i = last_i\n",
    "            continue\n",
    "\n",
    "        last_i = i\n",
    "        while alignment:\n",
    "            skip_features = False\n",
    "            if left_overlap:\n",
    "                 ### Debug ###\n",
    "                #print(dico[token.text])\n",
    "                #print(dico[token_object[index-1].text])\n",
    "                #print(token.text, token_object[index-1].text)\n",
    "                #print(token, label_obj[i])\n",
    "                #print(token_object[index-1].text)\n",
    "                #print(left_overlap)\n",
    "\n",
    "                try: \n",
    "                    if vowel_final(word_text_object[index-1].text, pronunciation): \n",
    "                        dico[token.text][\"Syl\"+str(syl_number)] = \"FUSED\"\n",
    "                        skip_features = True\n",
    "\n",
    "                    ### Handles rare cases of triple fusions betweeen syllables\n",
    "\n",
    "                    elif dico[token_object[index-1].text][\"Syl1\"] == \"FUSED\" and \"Syl2\" not in dico[token_object[index-1].text]:\n",
    "                        dico[token.text][\"Syl\"+str(syl_number)] = \"FUSED\"\n",
    "                        skip_features = True\n",
    "\n",
    "                    elif \"Syl1AlignBegin\" in dico[token_object[index-1].text] and str(dico[token_object[index-1].text]['Syl1AlignBegin']) == str(int(label_obj[i].start_time*1000)):\n",
    "                        dico[token.text][\"Syl\"+str(syl_number)] = \"FUSED\"\n",
    "                        skip_features = True\n",
    "\n",
    "                    else:\n",
    "                        dico[token.text][\"Syl\"+str(syl_number)+\"ExternalOnset\"] = \"True\"\n",
    "                except:\n",
    "                    print(\"problem on {}, confirm output\".format(label_obj[i]))\n",
    "\n",
    "                #dico[token.text][\"LeftOverlap\"] = \"True\"\n",
    "                \n",
    "            if right_overlap:\n",
    "                if vowel_final(word_text_object[index].text, pronunciation) != True and label_obj[i].start_time > token.start_time:  \n",
    "                    skip_features = True\n",
    "\n",
    "                #dico[token.text][\"RightOverlap\"] = \"True\"\n",
    "                \n",
    "            if skip_features == False:\n",
    "            \n",
    "                if label_obj[i].text:\n",
    "                    dico[token.text][\"Syl\"+str(syl_number)+\"Glo\"] = label_obj[i].text\n",
    "                    dico[token.text][\"Syl\"+str(syl_number)+\"Loc\"] = label_obj2[i].text\n",
    "                    dico[token.text][\"Syl\"+str(syl_number)+\"Duration\"] = str(int((label_obj[i].end_time - label_obj[i].start_time)*1000))\n",
    "                    \n",
    "                    feature_dico = contourToFeatures(label_obj[i].text, \"Glo\")\n",
    "                    feature_dico.update(contourToFeatures(label_obj2[i].text, \"Loc\"))\n",
    "\n",
    "                    for feature in feature_dico.keys():\n",
    "                        dico[token.text][\"Syl\"+str(syl_number)+feature] = feature_dico[feature]\n",
    "                    dico[token.text][\"SyllableCount\"] = str(syl_number)\n",
    "\n",
    "                    #dico[token.text][\"SylStart\"] = label_obj[i].start_time\n",
    "\n",
    "                else:\n",
    "                    if \"#\" not in word_text_object[index].text:\n",
    "                        dico[token.text][\"Syl\"+str(syl_number)+\"Glo\"] = \"X\"\n",
    "                        dico[token.text][\"Syl\"+str(syl_number)+\"Loc\"] = \"X\"\n",
    "                        #dico[token.text][\"SylStart\"] = label_obj[i].start_time\n",
    "                    \n",
    "                if \"#\" not in word_text_object[index].text:\n",
    "                    dico[token.text][\"Syl\"+str(syl_number)+\"AlignBegin\"] = str(int(label_obj[i].start_time*1000))\n",
    "                    dico[token.text][\"Syl\"+str(syl_number)+\"AlignEnd\"] = str(int(label_obj[i].end_time*1000))\n",
    "\n",
    "                    dico[token.text][\"Syl\"+str(syl_number)] = syllable_transcription_object[i].text\n",
    "\n",
    "\n",
    "            syl_number+=1\n",
    "            i+=1\n",
    "            if i == len(label_obj):\n",
    "                break\n",
    "                \n",
    "            alignment, left_overlap, right_overlap = confirm_alignment(token, label_obj[i])\n",
    "            #print(alignment, left_overlap, right_overlap)\n",
    "\n",
    "        \n",
    "        i-=1\n",
    "        \n",
    "\n",
    "                \n",
    "    return dico\n",
    "\n",
    "def translate_code(code):\n",
    "    \"\"\"Takes in entry a textual SLAM label and converts it into a list of numeric values to facilitate extraction of features\"\"\"\n",
    "\n",
    "    translated_code = []\n",
    "    for letter in code[0:2]:\n",
    "        if letter == \"L\":\n",
    "            translated_code.append(1)\n",
    "        elif letter == \"l\":\n",
    "            translated_code.append(2)\n",
    "        elif letter == \"m\":\n",
    "            translated_code.append(3)\n",
    "        elif letter == \"h\":\n",
    "            translated_code.append(4)\n",
    "        elif letter == \"H\":\n",
    "            translated_code.append(5)\n",
    "\n",
    "    if len(code) > 2:\n",
    "        if code[2] == \"L\":\n",
    "            translated_code.append((1, int(code[3])))\n",
    "        elif code[2] == \"l\":\n",
    "            translated_code.append((2, int(code[3])))\n",
    "        elif code[2] == \"m\":\n",
    "            translated_code.append((3, int(code[3])))\n",
    "        elif code[2] == \"h\":\n",
    "            translated_code.append((4, int(code[3])))\n",
    "        elif code[2] == \"H\":\n",
    "            translated_code.append((5, int(code[3])))\n",
    "\n",
    "    return translated_code\n",
    "            \n",
    "def contourToFeatures(contour_label, suffix=\"\"):\n",
    "    \"\"\"Takes as entry a textual SLAM label and returns a set of categorical prosodic features describing the label.\"\"\"\n",
    "    \n",
    "    dico = {}\n",
    "\n",
    "    code = translate_code(contour_label)\n",
    "\n",
    "    if code[0] == code[1]:\n",
    "        dico[\"Slope\"+suffix] = \"Flat\"\n",
    "    elif code[0] < code[1]:\n",
    "        dico[\"Slope\"+suffix] = \"Rise\"\n",
    "    elif code[0] > code[1]:\n",
    "        dico[\"Slope\"+suffix] = \"Fall\"\n",
    "\n",
    "    if len(code) == 2:\n",
    "        height = (code[0] + code[1]) / 2\n",
    "    if len(code) == 3:\n",
    "        height = (code[0] + code[1] + code[2][0]) / 3\n",
    "\n",
    "    if height >= 3.5:\n",
    "        dico[\"AvgHeight\"+suffix] = \"H\"\n",
    "    elif height < 2.5:\n",
    "        dico[\"AvgHeight\"+suffix] = \"L\"\n",
    "    else: \n",
    "        dico[\"AvgHeight\"+suffix] = \"M\"\n",
    "\n",
    "    amplitude = abs(code[0] - code[1])\n",
    "    if amplitude <= 1:\n",
    "        dico[\"PitchRange\"+suffix] = \"L\"\n",
    "    elif amplitude >= 3:\n",
    "        dico[\"PitchRange\"+suffix] = \"H\"\n",
    "    else: \n",
    "        dico[\"PitchRange\"+suffix] = \"M\"\n",
    "        \n",
    "    return dico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_trees_and_metadata(file_path: str) -> tuple:\n",
    "    \"\"\"Extracts trees and their metadata from a CoNLL file.\"\"\"\n",
    "    trees = conll.conllFile2trees(file_path)\n",
    "    file_name = os.path.basename(file_path)\n",
    "\n",
    "    metadata = []\n",
    "    for tree in trees:\n",
    "        tree_str = str(tree)\n",
    "        sent_id_match = re.search(r\"# sent_id = (.+)\", tree_str)\n",
    "        sent_id = sent_id_match.group(1) if sent_id_match else \"_\"\n",
    "        words = tree.words\n",
    "        metadata.append((tree, sent_id, words))\n",
    "\n",
    "    return trees, file_name, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_values_from_pitchtier(\n",
    "    file_content: list, align_begin: int, align_end: int\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Extracts pitch values from a pitchtier file.\n",
    "    \n",
    "    Takes as entry:\n",
    "    * A list of lines from a pitchtier file\n",
    "    * Two integers representing the beginning and end of the alignment interval\n",
    "    \"\"\"\n",
    "    values = []\n",
    "    numbers = []\n",
    "\n",
    "    for ligne in file_content:\n",
    "        if \"number =\" in ligne:\n",
    "            number = float(ligne.split(\"=\")[1])\n",
    "        elif \"value =\" in ligne:\n",
    "            value = float(ligne.split(\"=\")[1])\n",
    "            if align_begin <= number <= align_end:\n",
    "                values.append(value)\n",
    "                numbers.append(number)\n",
    "    return values, numbers\n",
    "\n",
    "def is_number(s: str) -> bool:\n",
    "    \"\"\"Checks if a string can be converted into a float.\"\"\"\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "def extract_pitchtier_infos(sent_begin: float, sent_end: float, tok_tree: dict, pitchtier_file: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts pitch values from a pitchtier file for a given sentence.\n",
    "    \n",
    "    Takes as entry:\n",
    "    * The beginning and end of the alignment interval for the sentence\n",
    "    * A token tree\n",
    "    * The path to a pitchtier file\n",
    "    \"\"\"\n",
    "    sent_align_begin = sent_begin\n",
    "    sent_align_end = sent_end\n",
    "    file_content = []\n",
    "    sent_form_values = []\n",
    "    sent_form_numbers = []\n",
    "\n",
    "    # Ensure sent_align_begin and sent_align_end are floats\n",
    "    if sent_align_begin and sent_align_end and is_number(sent_align_begin) and is_number(sent_align_end):\n",
    "        # print(\"sent_align_begin\", sent_align_begin)\n",
    "        sent_align_begin = float(sent_align_begin) / 1000\n",
    "        sent_align_end = float(sent_align_end) / 1000\n",
    "\n",
    "        with open(pitchtier_file, \"r\") as file:\n",
    "            file_content = file.readlines()\n",
    "\n",
    "        sent_form_values, sent_form_numbers = extract_values_from_pitchtier(\n",
    "            file_content, sent_align_begin, sent_align_end\n",
    "        )\n",
    "\n",
    "    syl_align_begin_values = []\n",
    "    syl_align_end_values = []\n",
    "    Syls_infos = {}\n",
    "\n",
    "    misc_dict = build_feature_dico(tok_tree[\"misc\"])\n",
    "\n",
    "    for i in range(1, 9):\n",
    "        syl_key = f\"Syl{i}\"\n",
    "        if syl_key in tok_tree[\"misc\"]:\n",
    "            align_begin_key = f\"{syl_key}AlignBegin\"\n",
    "            align_end_key = f\"{syl_key}AlignEnd\"\n",
    "            if align_begin_key in misc_dict and misc_dict[align_begin_key] != \"_\":\n",
    "                syl_align_begin_values.append(float(misc_dict[align_begin_key]) / 1000)\n",
    "                syl_align_end_values.append(float(misc_dict[align_end_key]) / 1000)\n",
    "\n",
    "    for i, (align_begin, align_end) in enumerate(\n",
    "        zip(syl_align_begin_values, syl_align_end_values)\n",
    "    ):\n",
    "        if align_begin > 0:\n",
    "            if file_content:  # Ensure file_content is not empty before processing\n",
    "                values, numbers = extract_values_from_pitchtier(\n",
    "                    file_content, align_begin, align_end\n",
    "                )\n",
    "                Syls_infos[f\"Syl{i+1}\"] = {\n",
    "                    \"Alignbegin\": align_begin,\n",
    "                    \"Alignend\": align_end,\n",
    "                    \"values\": values,\n",
    "                    \"numbers\": numbers,\n",
    "                }\n",
    "\n",
    "    sent_pitchtier_infos = {\n",
    "        \"Alignbegin\": sent_align_begin,\n",
    "        \"Alignend\": sent_align_end,\n",
    "        \"values\": sent_form_values,\n",
    "        \"numbers\": sent_form_numbers,\n",
    "    }\n",
    "\n",
    "    return Syls_infos, sent_pitchtier_infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semitones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semitones_between(frequency1: float, frequency2: float) -> float:\n",
    "    \"\"\"Calculates the number of semitones between two frequencies.\"\"\"\n",
    "    if frequency1 <= 0 or frequency2 <= 0:\n",
    "        return None\n",
    "\n",
    "    if frequency1 == frequency2:\n",
    "        return 0\n",
    "\n",
    "    ratio = frequency1 / frequency2\n",
    "    semitones = 12 * math.log2(ratio)\n",
    "\n",
    "    return round(semitones, 3)\n",
    "\n",
    "def hertz_semiton_data(syls_infos: dict, sent_pitchtier_infos: dict, token_data) -> dict:\n",
    "    \"\"\"Calculates the mean F0 and semitones between syllables and sentence for a given token.\"\"\"\n",
    "    sent_form_values = sent_pitchtier_infos[\"values\"]\n",
    "    sent_form_numbers = sent_pitchtier_infos[\"numbers\"]\n",
    "    moyenne_sent = (\n",
    "        sum(sent_form_values) / len(sent_form_values) if sent_form_values else 0\n",
    "    )\n",
    "\n",
    "    for syl_key, syl_data in syls_infos.items():\n",
    "        values = syl_data.get(\"values\", [])\n",
    "        if values:\n",
    "            moyenne_syl_hertz = sum(values) / len(values)\n",
    "            syl_data[\"MeanF0\"] = round(moyenne_syl_hertz, 3)\n",
    "            syl_data[\"SemitonesFromUtteranceMean\"] = semitones_between(\n",
    "                moyenne_syl_hertz, moyenne_sent\n",
    "            )\n",
    "        else:\n",
    "            syl_data[\"MeanF0\"] = 0\n",
    "            syl_data[\"SemitonesFromUtteranceMean\"] = 0\n",
    "\n",
    "    sent_pitchtier_infos[\"MoyenneSentHertz\"] = moyenne_sent\n",
    "\n",
    "    if \"root\" in token_data.values():\n",
    "        f0_enonce = float(moyenne_sent)\n",
    "        f0_enonce_rounded = round(f0_enonce, 3)\n",
    "        sent_pitchtier_infos[\"UtteranceMeanF0\"] = f0_enonce_rounded\n",
    "\n",
    "    return syls_infos, sent_pitchtier_infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slope_data(syls_infos: dict) -> dict:\n",
    "    \"\"\"Calculates the slope of the glissando between the first and last syllables of a token.\"\"\"\n",
    "    for syl_key, syl_data in syls_infos.items():\n",
    "        sec = syl_data[\"numbers\"]\n",
    "        hertz = syl_data[\"values\"]\n",
    "\n",
    "        if sec and hertz:\n",
    "            coordonnee = (sec[0], sec[-1], hertz[0], hertz[-1])\n",
    "            seuil_glissando = 0.16 / (sec[-1] - sec[0]) if sec[-1] - sec[0] != 0 else 0\n",
    "            semiton = semitones_between(hertz[0], hertz[-1])\n",
    "\n",
    "            if abs(semiton) > seuil_glissando and semiton > 0:\n",
    "                slope = \"Rise\"\n",
    "            elif abs(semiton) > seuil_glissando and semiton < 0:\n",
    "                slope = \"Fall\"\n",
    "            elif semiton == 0 and seuil_glissando == 0:\n",
    "                slope = \"X\"\n",
    "            else:\n",
    "                slope = \"Flat\"\n",
    "\n",
    "            syl_data[f\"Coordonnee\"] = coordonnee\n",
    "            syl_data[f\"Semiton\"] = semiton\n",
    "            syl_data[f\"Slope\"] = slope\n",
    "        else:\n",
    "            syl_data[f\"Coordonnee\"] = \"X\"\n",
    "            syl_data[f\"Semiton\"] = \"X\"\n",
    "            syl_data[f\"Slope\"] = \"X\"\n",
    "\n",
    "    return syls_infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amplitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_amplitudes(audio_file: str, start_time: float, end_time: float):\n",
    "    \"\"\"Calculates the maximum and average amplitudes between two time points in an audio file.\"\"\"\n",
    "    try:\n",
    "        # Ensure the audio file path is a string, not a list\n",
    "        if isinstance(audio_file, list):\n",
    "            audio_file = audio_file[0]\n",
    "        \n",
    "        sound = parselmouth.Sound(audio_file)\n",
    "        \n",
    "        # Ensure start_time and end_time are within the duration of the sound\n",
    "        if start_time < 0 or end_time > sound.get_total_duration():\n",
    "            raise ValueError(\"Start time or end time is out of the sound file's duration.\")\n",
    "        \n",
    "        segment = sound.extract_part(from_time=start_time, to_time=end_time)\n",
    "        \n",
    "        amplitudes = segment.to_intensity().values.T\n",
    "        max_amp = np.max(amplitudes)\n",
    "        avg_amp = np.mean(amplitudes)\n",
    "        \n",
    "        return round(max_amp, 3), round(avg_amp, 3)\n",
    "    except Exception as e:\n",
    "        # print(\"Error calculating amplitudes:\", e)\n",
    "        return 0, 0\n",
    "\n",
    "\n",
    "def amplitude_data(syls_infos: dict, sent_infos: dict, audio_file: str):\n",
    "    \"\"\"Calculates the maximum and average amplitudes for syllables and sentences in a token.\"\"\"\n",
    "    token_start_time = None\n",
    "    token_end_time = None\n",
    "    \n",
    "    for syl_key, syl_data in syls_infos.items():\n",
    "        if \"Alignbegin\" in syl_data and \"Alignend\" in syl_data:\n",
    "            if token_start_time is None:\n",
    "                token_start_time = syl_data[\"Alignbegin\"]\n",
    "            token_end_time = syl_data[\"Alignend\"]\n",
    "\n",
    "            start_time = syl_data[\"Alignbegin\"]\n",
    "            end_time = syl_data[\"Alignend\"]\n",
    "            \n",
    "            result = calculate_amplitudes(audio_file, start_time, end_time)\n",
    "            if result:\n",
    "                max_amp, avg_amp = result\n",
    "                syl_data[\"MaxAmplitude\"] = max_amp\n",
    "                syl_data[\"AvgAmplitude\"] = avg_amp\n",
    "            else:\n",
    "                print(f\"Failed to calculate amplitudes for syllable {syl_key}\")\n",
    "\n",
    "    if token_start_time is not None and token_end_time is not None:\n",
    "        result = calculate_amplitudes(audio_file, token_start_time, token_end_time)\n",
    "        if result:\n",
    "            max_amp, avg_amp = result\n",
    "            sent_infos[\"TokenMaxAmplitude\"] = max_amp\n",
    "            sent_infos[\"TokenAvgAmplitude\"] = avg_amp\n",
    "        else:\n",
    "            print(\"Failed to calculate amplitudes for the entire token\")\n",
    "\n",
    "    return syls_infos, sent_infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronunciation = cmudict.dict() # Pronunciation dictionary: to modify according to the language chosen\n",
    "syl_tier = \"SyllablesStyleGlo\"  # Tier containing global contour\n",
    "syl_tier2 = \"SyllablesStyleLoc\"  # Tier containing local contour\n",
    "word_tier = \"Word-ID\"  # Tier containing numeric ID for token\n",
    "word_text_tier = \"Word-Text\"  # Tier containing text token text\n",
    "syllable_text_tier = \"Syllables\"  # Tier containing syllabic transcriptions\n",
    "slam_files = glob.glob(\"SLAM_output/*.TextGrid\")  # Folder containing SLAM labels in TextGrid format\n",
    "conll_infiles = glob.glob(\"CONLL_files/*.conllu\")  # Folder containing CONLLU files to which prosodic information will be added\n",
    "\n",
    "conll_outfolder = \"CONLL_outfiles/\" # Folder where the new CONLLU files will be saved\n",
    "\n",
    "pitchtier_infiles = glob.glob(\"PITCHTIER/*.PitchTier\")  # Folder containing pitchtier files\n",
    "audio_infiles = glob.glob(\"WAV/*.wav\")  # Folder containing audio files\n",
    "\n",
    "# If running this script a second time, useful for renaming features\n",
    "feature_rename_dict = {\"OldFeatname\": \"NewFeatname\"}\n",
    "\n",
    "if not os.path.exists(conll_outfolder):\n",
    "    os.makedirs(conll_outfolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill CONLLU files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for slam_file in sorted(slam_files):\n",
    "    basename = os.path.basename(slam_file)[:-9]\n",
    "    pitchtier_files = [file for file in pitchtier_infiles if basename in file]\n",
    "    if not pitchtier_files:\n",
    "        # print(f\"No pitchtier file found for {basename}\")\n",
    "        continue\n",
    "    pitchtier_file = pitchtier_files[0]\n",
    "\n",
    "    audio_file = [file for file in audio_infiles if basename in file]\n",
    "    if not audio_file:\n",
    "        # print(f\"No audio file found for {basename}\")\n",
    "        continue\n",
    "\n",
    "    print(\"treating\", basename)\n",
    "    annotations = extractProsodicAnnotation(\n",
    "        slam_file,\n",
    "        \"SyllablesStyleGlo\",\n",
    "        \"SyllablesStyleLoc\",\n",
    "        \"Word-ID\",\n",
    "        \"Word-Text\",\n",
    "        \"Syllables\",\n",
    "    )\n",
    "    \n",
    "    # list of conllu files to be treated\n",
    "    conllu_outfiles = []\n",
    "\n",
    "    for infile in conll_infiles:\n",
    "        \n",
    "        if os.path.basename(infile)[:len(basename)] == basename:\n",
    "            trees, _, metadata = extract_trees_and_metadata(infile)\n",
    "            for treei, (tree, _, _) in enumerate(metadata):\n",
    "                for token in tree:\n",
    "                    identifier = str(treei+1)+\":\"+str(token)\n",
    "                    misc_features = tree[token]['misc']\n",
    "                    feature_dico = build_feature_dico(misc_features)\n",
    "                    \n",
    "                    \n",
    "                    features_to_delete = []\n",
    "                    \n",
    "                    new_feature_dico = {}\n",
    "                    for feature in feature_dico.keys():\n",
    "                        #print(feature)\n",
    "                        if re.match(\"Syl[0-9].*\", feature):\n",
    "                            if \"Amplitude\" not in feature:\n",
    "                                features_to_delete.append(feature)\n",
    "                            #print(features_to_delete)\n",
    "                            \n",
    "                        #print(feature)\n",
    "                        if feature[4:] in feature_rename_dict.keys():\n",
    "                            newfeat = feature.replace(feature[4:], feature_rename_dict[feature[4:]])\n",
    "                            #print(feature, newfeat)\n",
    "                            newval = feature_dico[feature]\n",
    "                            new_feature_dico[newfeat] = newval\n",
    "                            features_to_delete.append(feature)\n",
    "                            #print(feature_dico)\n",
    "\n",
    "                        \"\"\"\"   \n",
    "                        if feature == \"F0Enonce\":\n",
    "                            newfeat = feature.replace(feature, feature_rename_dict[feature])\n",
    "                            #print(feature, newfeat)\n",
    "                            newval = feature_dico[feature]\n",
    "                            new_feature_dico[newfeat] = newval\n",
    "                            features_to_delete.append(feature)\n",
    "                            #print(feature_dico)\n",
    "                        \"\"\"\n",
    "                            \n",
    "                    for feature in new_feature_dico.keys():\n",
    "                        feature_dico[feature] = new_feature_dico[feature]\n",
    "                        \n",
    "                    \n",
    "                    for feature in features_to_delete:\n",
    "                        if feature in feature_dico.keys():\n",
    "                            del feature_dico[feature]\n",
    "                    #print(feature_dico)\n",
    "                    \n",
    "                        \n",
    "                    #print(feature_dico)\n",
    "                    if identifier in annotations.keys():\n",
    "                        for item in annotations[identifier]:\n",
    "                            #print(identifier, item)\n",
    "                            feature_dico[item] = annotations[identifier][item]\n",
    "                        \n",
    "                        if \"Syl1\" in feature_dico.keys() and feature_dico[\"Syl1\"] == \"FUSED\":\n",
    "                            #print(\"hi\")\n",
    "                            if \"Syl1Duration\" in feature_dico.keys():\n",
    "                                del feature_dico[\"Syl1Duration\"]\n",
    "                            if \"Syl1MeanF0\" in feature_dico.keys():\n",
    "                                del feature_dico[\"Syl1MeanF0\"]\n",
    "                            if \"Syl1SemitonesFromUtteranceMean\" in feature_dico.keys():\n",
    "                                del feature_dico[\"Syl1SemitonesFromUtteranceMean\"]\n",
    "                        #print(identifier)\n",
    "                        #print(feature_string)\n",
    "                        feature_string = dico2FeatureString(feature_dico)\n",
    "                        tree[token]['misc'] = feature_string\n",
    "                    \n",
    "                    \"\"\"\n",
    "                    identifier = str(treei+1)+\":\"+str(token)\n",
    "                    misc_features = tree[token]['misc']\n",
    "                    \n",
    "                    if identifier in annotations.keys():              \n",
    "                        for item in annotations[identifier]:\n",
    "                            misc_features = misc_features + \"|\"\n",
    "                            featstring = item+\"=\"+annotations[identifier][item]\n",
    "                            misc_features = misc_features + featstring\n",
    "                    tree[token]['misc'] = misc_features\n",
    "                    #print(tree[token]['misc'])\n",
    "                    \"\"\"\n",
    "\n",
    "            output_file = os.path.join(conll_outfolder, os.path.basename(infile))\n",
    "            conll.trees2conllFile(trees, output_file)\n",
    "            conllu_outfiles.append(output_file)\n",
    "            \n",
    "    for infile in conllu_outfiles:\n",
    "        if os.path.basename(infile)[: len(basename)] == basename:\n",
    "            # print(\"CoNLL outfile :\", infile)\n",
    "            trees, _, metadata = extract_trees_and_metadata(infile)\n",
    "\n",
    "            for treei, (tree, _, _) in enumerate(metadata):\n",
    "                tokens = []\n",
    "\n",
    "                for tok in tree:\n",
    "                    tokens.append(tree[tok])\n",
    "                    feature_dico = build_feature_dico(tree[tok][\"misc\"])\n",
    "\n",
    "                if tokens:\n",
    "                    sent_align_begin = tokens[0].get(\"misc\", \"\").split(\"|\")[0].split(\"=\")[1]\n",
    "                    sent_align_end = tokens[-1].get(\"misc\", \"\").split(\"|\")[1].split(\"=\")[1]\n",
    "\n",
    "                    for token_data in tokens:\n",
    "                        feature_dico = build_feature_dico(token_data[\"misc\"])\n",
    "                        if \"PUNCT\" not in token_data[\"tag\"]:\n",
    "                            syls_infos, sent_infos = extract_pitchtier_infos(\n",
    "                                sent_align_begin,\n",
    "                                sent_align_end,\n",
    "                                token_data,\n",
    "                                pitchtier_file,\n",
    "                            )\n",
    "\n",
    "                            syls_infos, sent_infos = hertz_semiton_data(syls_infos, sent_infos, token_data[\"gov\"])\n",
    "                            syls_infos = slope_data(syls_infos)\n",
    "                            syls_infos, sent_infos = amplitude_data(syls_infos, sent_infos, audio_file)\n",
    "\n",
    "                            for syl_key, syl_info in syls_infos.items():\n",
    "                                for key, value in syl_info.items():\n",
    "                                    if key not in [\"Coordonnee\", \"values\", \"numbers\", \"Alignbegin\", \"Alignend\"]:\n",
    "                                        feature_dico[f\"{syl_key}{key}\"] = value\n",
    "\n",
    "                            if token_data[\"gov\"] == {0: \"root\"}:\n",
    "                                if \"MaxAmplitude\" in sent_infos:\n",
    "                                    feature_dico[\"MaxAmplitude\"] = sent_infos[\"MaxAmplitude\"]\n",
    "                                if \"AvgAmplitude\" in sent_infos:\n",
    "                                    feature_dico[\"AvgAmplitude\"] = sent_infos[\"AvgAmplitude\"]\n",
    "                                if \"UtteranceMeanF0\" in sent_infos:\n",
    "                                    feature_dico[\"UtteranceMeanF0\"] = sent_infos[\"UtteranceMeanF0\"]\n",
    "\n",
    "                        feature_string = dico2FeatureString(feature_dico)\n",
    "                        token_data[\"misc\"] = feature_string\n",
    "\n",
    "            conll.trees2conllFile(trees, infile)\n",
    "\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stage_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
